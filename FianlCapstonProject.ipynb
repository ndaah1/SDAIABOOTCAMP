{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train violence classification Datasets Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\96659\\miniconda3\\lib\\site-packages (10.3.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pillow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 files belonging to 2 classes.\n",
      "Found 477 files belonging to 2 classes.\n",
      "Found 356 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    'Violencenotviolence/train',\n",
    "    batch_size=64,\n",
    "\n",
    "    image_size=(224, 224),\n",
    "    pad_to_aspect_ratio=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    'Violencenotviolence/valid',\n",
    "    batch_size=64,\n",
    "\n",
    "    image_size=(224, 224),\n",
    "    pad_to_aspect_ratio=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    'Violencenotviolence/test',\n",
    "    batch_size=64,\n",
    "\n",
    "    image_size=(224, 224),\n",
    "    pad_to_aspect_ratio=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,719,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,050</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m4,719,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m2,050\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,290,242</span> (24.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,290,242\u001b[0m (24.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,290,242</span> (24.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,290,242\u001b[0m (24.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the units argument to the Dense layer\n",
    "model = keras.models.Sequential([\n",
    "    L.Input(shape=(224, 224, 3)),  # Change the input shape\n",
    "    L.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.Conv2D(filters=128, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.Conv2D(filters=256, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.Conv2D(filters=512, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.Conv2D(filters=1024, kernel_size=3, strides=2, activation='relu'),\n",
    "    L.GlobalAveragePooling2D(),\n",
    "    L.Dense(units=2, activation='softmax'),  # Add the units argument\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 814ms/step - accuracy: 0.4971 - loss: 28.2221 - val_accuracy: 0.4570 - val_loss: 0.7989 - learning_rate: 0.0010\n",
      "Epoch 2/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 765ms/step - accuracy: 0.4872 - loss: 0.7469 - val_accuracy: 0.5472 - val_loss: 0.6676 - learning_rate: 0.0010\n",
      "Epoch 3/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 736ms/step - accuracy: 0.5778 - loss: 0.6700 - val_accuracy: 0.5220 - val_loss: 0.7643 - learning_rate: 0.0010\n",
      "Epoch 4/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 737ms/step - accuracy: 0.5191 - loss: 0.7351 - val_accuracy: 0.5660 - val_loss: 0.6495 - learning_rate: 0.0010\n",
      "Epoch 5/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 747ms/step - accuracy: 0.5333 - loss: 0.6711 - val_accuracy: 0.5849 - val_loss: 0.6399 - learning_rate: 0.0010\n",
      "Epoch 6/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 746ms/step - accuracy: 0.5893 - loss: 0.6533 - val_accuracy: 0.5786 - val_loss: 0.6420 - learning_rate: 0.0010\n",
      "Epoch 7/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 740ms/step - accuracy: 0.5856 - loss: 0.6486 - val_accuracy: 0.6205 - val_loss: 0.6367 - learning_rate: 0.0010\n",
      "Epoch 8/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 733ms/step - accuracy: 0.6172 - loss: 0.6389 - val_accuracy: 0.5933 - val_loss: 0.6420 - learning_rate: 0.0010\n",
      "Epoch 9/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 768ms/step - accuracy: 0.6588 - loss: 0.6075 - val_accuracy: 0.5681 - val_loss: 0.6486 - learning_rate: 0.0010\n",
      "Epoch 10/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 744ms/step - accuracy: 0.6236 - loss: 0.6179 - val_accuracy: 0.5744 - val_loss: 0.6903 - learning_rate: 0.0010\n",
      "Epoch 11/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 741ms/step - accuracy: 0.6410 - loss: 0.6095 - val_accuracy: 0.5954 - val_loss: 0.6588 - learning_rate: 0.0010\n",
      "Epoch 12/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 740ms/step - accuracy: 0.6459 - loss: 0.6062 - val_accuracy: 0.5702 - val_loss: 0.6532 - learning_rate: 0.0010\n",
      "Epoch 13/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 745ms/step - accuracy: 0.6454 - loss: 0.5657 - val_accuracy: 0.5828 - val_loss: 0.6587 - learning_rate: 1.0000e-04\n",
      "Epoch 14/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 752ms/step - accuracy: 0.7197 - loss: 0.5465 - val_accuracy: 0.6164 - val_loss: 0.6803 - learning_rate: 1.0000e-04\n",
      "Epoch 15/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 760ms/step - accuracy: 0.7649 - loss: 0.5241 - val_accuracy: 0.6310 - val_loss: 0.6830 - learning_rate: 1.0000e-04\n",
      "Epoch 16/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 765ms/step - accuracy: 0.7605 - loss: 0.5127 - val_accuracy: 0.6331 - val_loss: 0.6713 - learning_rate: 1.0000e-04\n",
      "Epoch 17/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 794ms/step - accuracy: 0.7712 - loss: 0.4936 - val_accuracy: 0.6436 - val_loss: 0.6774 - learning_rate: 1.0000e-04\n",
      "Epoch 18/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 750ms/step - accuracy: 0.7979 - loss: 0.4791 - val_accuracy: 0.6331 - val_loss: 0.6786 - learning_rate: 1.0000e-05\n",
      "Epoch 19/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 736ms/step - accuracy: 0.7749 - loss: 0.4879 - val_accuracy: 0.6415 - val_loss: 0.6787 - learning_rate: 1.0000e-05\n",
      "Epoch 20/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 768ms/step - accuracy: 0.7883 - loss: 0.4873 - val_accuracy: 0.6415 - val_loss: 0.6794 - learning_rate: 1.0000e-05\n",
      "Epoch 21/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 769ms/step - accuracy: 0.7916 - loss: 0.4862 - val_accuracy: 0.6373 - val_loss: 0.6799 - learning_rate: 1.0000e-05\n",
      "Epoch 22/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 764ms/step - accuracy: 0.7825 - loss: 0.4884 - val_accuracy: 0.6436 - val_loss: 0.6804 - learning_rate: 1.0000e-05\n",
      "Epoch 23/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 782ms/step - accuracy: 0.7761 - loss: 0.4842 - val_accuracy: 0.6415 - val_loss: 0.6805 - learning_rate: 1.0000e-06\n",
      "Epoch 24/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 778ms/step - accuracy: 0.7782 - loss: 0.4819 - val_accuracy: 0.6415 - val_loss: 0.6806 - learning_rate: 1.0000e-06\n",
      "Epoch 25/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 790ms/step - accuracy: 0.7712 - loss: 0.4745 - val_accuracy: 0.6436 - val_loss: 0.6808 - learning_rate: 1.0000e-06\n",
      "Epoch 26/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 774ms/step - accuracy: 0.7488 - loss: 0.4897 - val_accuracy: 0.6415 - val_loss: 0.6809 - learning_rate: 1.0000e-06\n",
      "Epoch 27/70\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 785ms/step - accuracy: 0.7618 - loss: 0.4814 - val_accuracy: 0.6415 - val_loss: 0.6811 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "log = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=70,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            patience=5,\n",
    "            factor=0.1,\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(log_dir='./logs/my_cnn')\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - accuracy: 0.7384 - loss: 0.6306\n",
      "Validation accuracy: 0.6205450892448425\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print('Validation accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('violence.keras')  # Save the model Keras F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Fall & Dangerous & Fight Detection Datasets Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> transform Dangerous Datasets from segmention to detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def polygon_to_box(annotation):\n",
    "    output = ''\n",
    "    for line in annotation.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        class_, *rest = line.split()\n",
    "        arr = np.asarray(list(map(float, rest))).reshape(-1, 2)\n",
    "        (x_min, y_min), (x_max, y_max) = arr.min(axis=0), arr.max(axis=0)\n",
    "        width, height = x_max - x_min, y_max - y_min\n",
    "        x_cen, y_cen = x_min + width / 2, y_min + height / 2\n",
    "        output += f'{class_} {x_cen:.6f} {y_cen:.6f} {width:.6f} {height:.6f}\\n'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "in_dir = Path('C:/Users/96659/OneDrive/Desktop/Mo/FallightDang/detectDangerous')\n",
    "out_dir = Path('./Dangerous')\n",
    "\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "for path in tqdm(list(in_dir.glob('*/labels/*.txt'))):\n",
    "    # read + convert + write the annotations\n",
    "    with open(path, mode='r') as file:\n",
    "        annotation = file.read()\n",
    "    annotation = polygon_to_box(annotation)\n",
    "    split, labels, file_name = path.parts[-3:]\n",
    "    target = out_dir / split / labels\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    with open(target / file_name, mode='w') as file:\n",
    "        file.write(annotation)\n",
    "\n",
    "    # copy image the images\n",
    "    source = in_dir / split / 'images'\n",
    "    image_path = next(source.glob(path.stem + '.*'))\n",
    "    target = out_dir / split / 'images'\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copyfile(image_path, target / image_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combination all datasets in one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path('./')\n",
    "out_dir = root_dir / 'CombinedDataset'\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(out_dir)\n",
    "datasets = {\n",
    "    'Dangerous': {0: 0, 1: 1, 2: 2},\n",
    "    'FallDetection': {0: 3},\n",
    "    'Fight': {0: 4, 1: 5},\n",
    "}\n",
    "\n",
    "for split in ('train', 'test', 'valid'):\n",
    "    split_dir = out_dir / split\n",
    "    (split_dir / 'images').mkdir(parents=True)\n",
    "    (split_dir / 'labels').mkdir()\n",
    "    for dataset, classes in datasets.items():\n",
    "        images_dir = root_dir / dataset / split / 'images'\n",
    "        for i, image_path in enumerate(images_dir.iterdir()):\n",
    "            name = f'{dataset}{i}'\n",
    "            out_image_path = split_dir / 'images' / image_path.with_stem(name).name\n",
    "            shutil.copyfile(image_path, out_image_path)\n",
    "\n",
    "            label_path = image_path.parent.parent / 'labels' / f'{image_path.stem}.txt'\n",
    "            with open(label_path, mode='r') as file:\n",
    "                labels = file.readlines()\n",
    "            labels = [\n",
    "                f'{classes[int(line[:1])]}{line[1:]}'\n",
    "                for line in labels\n",
    "            ]\n",
    "            out_label_path = split_dir / 'labels' / label_path.with_stem(name).name\n",
    "            with open(out_label_path, mode='w') as file:\n",
    "                file.write(''.join(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create file.yaml to combination datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../DangerFightFall/data.yaml\n",
    "path: ./DangerFightFall\n",
    "\n",
    "train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "\n",
    "names:\n",
    "    0: gun\n",
    "    1: helmet\n",
    "    2: knife\n",
    "    3: fall\n",
    "    4: fight\n",
    "    5: no-fight\n",
    "\n",
    "roboflow1:\n",
    "  workspace: university-ai\n",
    "  project: detect-dangerous\n",
    "  version: 1\n",
    "  license: CC BY 4.0\n",
    "  url: https://universe.roboflow.com/university-ai/detect-dangerous/dataset/1\n",
    "\n",
    "roboflow2:\n",
    "  workspace: roboflow-universe-projects\n",
    "  project: fall-detection-ca3o8\n",
    "  version: 1\n",
    "  license: CC BY 4.0\n",
    "  url: https://universe.roboflow.com/roboflow-universe-projects/fall-detection-ca3o8/dataset/1\n",
    "\n",
    "roboflow3:\n",
    "  workspace: fd\n",
    "  project: fight-detection\n",
    "  version: 1\n",
    "  license: CC BY 4.0\n",
    "  url: https://universe.roboflow.com/fd/fight-detection/dataset/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train correct class CombinedDataset, accuracy = 60\n",
    "!yolo task=detect mode=train model=yolov8n.pt data=/content/drive/MyDrive/FinalProject/FFDData/Data/DangerFightFall/configration.yaml epochs=130 batch=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine - tune model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy =67\n",
    "!yolo cfg=/content/drive/MyDrive/FinalProject/FFDData/Data/DangerFightFall/default_copy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train people Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Datasets \n",
    "!yolo task=detect mode=train model=/content/drive/MyDrive/pepole/runs/detect/train8/weights/best.pt  data=/content/drive/MyDrive/pepole/peoplecounterv/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine -tune\n",
    "!yolo task=detect mode=train model=/content/drive/MyDrive/pepole/runs/detect/train14/weights/best.pt data=/content/drive/MyDrive/pepole/peoplecounterv/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import av\n",
    "# import cv2\n",
    "# import pafy\n",
    "# import keras\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from ultralytics import YOLO\n",
    "# import streamlit as st  # webrtc works on version 1.33\n",
    "# from streamlit_webrtc import WebRtcMode, webrtc_streamer\n",
    "# from ultralytics.utils.plotting import Annotator, Colors\n",
    "\n",
    "\n",
    "# st.set_page_config(page_title='YOLO', page_icon='🔥')\n",
    "# st.title(\"Welcome to my Streamlit App!\")\n",
    "\n",
    "# @st.cache_resource()\n",
    "# def load_yolo(file_path):\n",
    "#     return YOLO(file_path)\n",
    "\n",
    "# @st.cache_resource()\n",
    "# def load_keras(file_path):\n",
    "#     return keras.models.load_model(file_path)\n",
    "\n",
    "# def yolo_classifier(model):\n",
    "#     def wrapped(img, **kwargs):\n",
    "#         results = violence(img, **kwargs)[0]\n",
    "#         label = ''\n",
    "#         if results.names:\n",
    "#             label = results.names[results.probs.top1]\n",
    "#         return label\n",
    "#     return wrapped\n",
    "\n",
    "# def keras_classifier(model):\n",
    "#     def wrapped(img):\n",
    "#         probs = model.predict(np.array(img)[None], verbose=False)[0]\n",
    "#         return ['non_violence', 'violence'][np.argmax(probs)]\n",
    "#     return wrapped\n",
    "\n",
    "# def plot_bounding_boxes(yolo_result, colors=Colors()):\n",
    "#     \"\"\"Draw bounding boxes from yolo.predict() result\"\"\"\n",
    "#     image = yolo_result.orig_img[..., ::-1]\n",
    "#     if yolo_result.boxes:\n",
    "#         annotate = Annotator(np.ascontiguousarray(image))\n",
    "#         classes = yolo_result.boxes.cls.tolist()\n",
    "#         scores = yolo_result.boxes.conf.tolist()\n",
    "#         boxes = yolo_result.boxes.xyxy\n",
    "#         for box, class_, score in zip(boxes, classes, scores):\n",
    "#             tag = f\"{yolo_result.names[class_].title()}: {score:.0%}\"\n",
    "#             annotate.box_label(box, tag, colors(class_))\n",
    "#         image = annotate.result()\n",
    "#     return Image.fromarray(image)\n",
    "\n",
    "# #violence = yolo_classifier(load_yolo(\"./runsVio/classify/train/weights/best.pt\"))\n",
    "# violence = keras_classifier(load_keras(\"./violence.keras\"))\n",
    "# def violence_classification(img ):\n",
    "#     label = violence(img)\n",
    "#     img = cv2.putText(\n",
    "#         np.array(img),\n",
    "#         label,\n",
    "#         (20, 30),\n",
    "#         cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#         fontScale=1,\n",
    "#         color=(255, 0, 0) if label == 'violence' else (0, 255, 0),\n",
    "#         thickness=2,\n",
    "#         lineType=cv2.LINE_AA,\n",
    "#     )\n",
    "#     return Image.fromarray(img)\n",
    "\n",
    "# crowed = load_yolo(\"./runs_crowed/detect/train17/weights/best.pt\")\n",
    "# def crowd_counting(img):\n",
    "#     result = crowed(img, conf=confidence)[0]\n",
    "\n",
    "#     # Count the number of people detected\n",
    "#     num_people = len(result.boxes.xyxy)\n",
    "\n",
    "#     # Draw the count on the image\n",
    "#     # img = np.array(plot_bounding_boxes(result))\n",
    "#     img = np.array(img)\n",
    "#     img = cv2.putText(\n",
    "#         img,\n",
    "#         f'Number of People: {num_people}',\n",
    "#         (20, 80),\n",
    "#         cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#         fontScale=1,\n",
    "#         color=(255, 0, 0),\n",
    "#         thickness=2,\n",
    "#         lineType=cv2.LINE_AA,\n",
    "#     )\n",
    "#     return Image.fromarray(img)\n",
    "\n",
    "# fall = load_yolo(\"./runs_ffd/detect/Nano/weights/best.pt\")\n",
    "# def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "#     img = frame.to_image()\n",
    "#     img = crowd_counting(img)\n",
    "#     img = violence_classification(img)\n",
    "#     img = plot_bounding_boxes(fall(img, conf=confidence)[0])\n",
    "#     return av.VideoFrame.from_image(img)\n",
    "\n",
    "\n",
    "\n",
    "# confidence = st.sidebar.slider('Confidence Threshold', 0., 1., 0.5, step=0.05)\n",
    "# iou = st.sidebar.slider('IoU Threshold', 0., 1., 0.5, step=0.05)\n",
    "\n",
    "# # Add navigation links to the sidebar\n",
    "# selection = st.sidebar.radio(\"Select Source\", [\"Webcam\", \"Youtube\", \"Image\",\"Video\"])\n",
    "\n",
    "# # Display content based on the selected option\n",
    "# if selection == \"Webcam\":\n",
    "#     webrtc_streamer(\n",
    "#         key=\"camera\",\n",
    "#         mode=WebRtcMode.SENDRECV,\n",
    "#         media_stream_constraints={\n",
    "#             \"video\": True,\n",
    "#             \"audio\": False,\n",
    "#         },\n",
    "#         video_frame_callback=video_frame_callback,\n",
    "#     )\n",
    "\n",
    "# elif selection == \"Youtube\":\n",
    "#     video_url = st.text_input(\"Enter the YouTube video URL\")\n",
    "#     if video_url:\n",
    "#         frame_holder = st.empty()\n",
    "#         video = pafy.new(video_url)\n",
    "#         best = video.getbest(preftype=\"mp4\")\n",
    "#         container = av.open(best.url)\n",
    "#         stream = container.streams.video[0]\n",
    "\n",
    "#         height= stream.height\n",
    "#         width = stream.width\n",
    "#         output_size = (width, height)\n",
    "        \n",
    "#         # Create a video writer to save the processed frames\n",
    "#         fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#         video_writer = cv2.VideoWriter('output.mp4', fourcc, 30, output_size)\n",
    "        \n",
    "#         # Read and process each frame\n",
    "#         try:\n",
    "#             for frame in container.decode(video=0):\n",
    "#                 try:\n",
    "#                     img_array = frame.to_ndarray(format=\"rgb24\")\n",
    "#                     img = Image.fromarray(img_array)\n",
    "#                     img = crowd_counting(img)\n",
    "#                     img = violence_classification(img)\n",
    "#                     img = plot_bounding_boxes(fall(img, conf=confidence)[0])\n",
    "#                     frame_holder.image(img)\n",
    "                    \n",
    "#                     # Write the processed frame to the video file\n",
    "#                     video_writer.write(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing frame: {e}\")\n",
    "#                     continue\n",
    "#         except Exception as e:\n",
    "#             print('Error', e)\n",
    "\n",
    "#         # Release the video writer\n",
    "#         video_writer.release()\n",
    "#         container.close()\n",
    "\n",
    "# elif selection == \"Image\":\n",
    "#     uploaded_file = st.file_uploader(\"Choose an image file\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "#     if uploaded_file is not None:\n",
    "#         img = Image.open(uploaded_file).convert('RGB')\n",
    "#         img = crowd_counting(img)\n",
    "#         img = violence_classification(img)\n",
    "#         img = plot_bounding_boxes(fall(img, conf=confidence)[0])\n",
    "#         st.image(image=img, caption='processed Image', use_column_width=True)\n",
    "\n",
    "# elif selection == \"Video\":\n",
    "#     uploaded_video = st.file_uploader(\"Choose a video...\", type=[\"mp4\", \"avi\", \"mov\"])\n",
    "#     if uploaded_video is not None:\n",
    "#         st.video(uploaded_video)\n",
    "\n",
    "#         container = av.open(uploaded_video)\n",
    "#         stream = container.streams.video[0]\n",
    "\n",
    "#         height= stream.height\n",
    "#         width = stream.width\n",
    "#         output_size = (width, height)\n",
    "        \n",
    "#         fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#         video_writer = cv2.VideoWriter('output_processed.mp4', fourcc, 30, output_size)\n",
    "        \n",
    "#         frame_holder = st.empty()\n",
    "#         try:\n",
    "#             for frame in container.decode(video=0):\n",
    "#                 try:\n",
    "#                     img_array = frame.to_ndarray(format=\"rgb24\")\n",
    "#                     img = Image.fromarray(img_array)\n",
    "#                     img = crowd_counting(img)\n",
    "#                     img = violence_classification(img)\n",
    "#                     img = plot_bounding_boxes(fall(img, conf=confidence)[0])\n",
    "#                     frame_holder.image(img)\n",
    "#                     video_writer.write(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing frame: {e}\")\n",
    "#                     continue\n",
    "#         except Exception as e:\n",
    "#             print('Error', e)\n",
    "\n",
    "#         video_writer.release()\n",
    "#         container.close()\n",
    "#         st.success(\"Video processed and saved as 'output_processed.mp4'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_classifier(model):\n",
    "    def wrapped(img, **kwargs):\n",
    "        results = violence(img, **kwargs)[0]\n",
    "        label = ''\n",
    "        if results.names:\n",
    "            label = results.names[results.probs.top1]\n",
    "        return label\n",
    "    return wrapped\n",
    "\n",
    "def keras_classifier(model):\n",
    "    def wrapped(img):\n",
    "        #img_array = np.array(img)\n",
    "    return wrapped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
